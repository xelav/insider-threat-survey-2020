
### Методы представления пользовательского поведения

Синтетический набор данных разработанный подразделением CERT Carnegie
Melon University пользуется очень большой популярностью в исследованиях
на данную тему. Поэтому, по умолчанию, во всех приведенных статьях ниже
используется набор данных CERT версии 4.2. Также по умолчанию содержимое
писем, файлов и вебсайтов из данного набора данных не используется.\

В [@aldairiTrustAwareUnsupervised2019] применяется метод обучения
без учителя и задача обнаружения инсайдерских угроз ставится как задача
обнаружения аномалий. В этой работе сравниваются два классических метода
нахождения аномалий:\

1. Isolation Forest (Изолирующий лес) - разновидность алгоритма
    случайного леса, в котором строятся случайные бинарные решающие
    деревья. Аномальными объектами считаются те, которые часто
    оказываются в листьях с низкой глубиной

2. Метод опорных векторов для одного класса (One Class SVM)
    представляет собой обычный SVM, в котором все данные имеют
    положительную метку. При оптимизации модель пытается вместить как
    можно больше объектов в как можно меньшую гиперплоскость. В
    результате получается граница, по одну сторону которой максимально
    плотно упакованы наблюдения из тренировочной выборке, а по другую
    будут находиться аномальные значения.\

Работа проводилась на наборе данных CERT. Моделям на вход подавались
данные агрегированные по разным временным периодам - дням, месяцам,
полугодиям и годам. Также в качестве дополнительного признака
используется trust score (показатель доверия), который означает оценку
аномальности пользователя для предыдущего периода. Авторы показывают,
что этот признак заметно улучшает точность предсказаний моделей,
особенно в случаях моделей, в которых данные агрегируются по малым
периодам.\

При применении моделей машинного обучения качество моделей очень сильно
зависит от признаков, которые были вручную сгенерированы
исследователями. Однако в последнее время наблюдается очень большой
интерес к нейронным сетям, в том числе из-за того, что они способны
автоматически выучивать хорошее признаковое представление данных.
Поэтому и в данной теме множество последних работ использует нейросети.\

В статье [@luInsiderThreatDetection2019] применяется рекуррентная
нейронная сеть LSTM для поиска аномалий в поведении пользователей.\

**Long Short Term Memory (LSTM)** - разновидность рекуррентных нейронных
сетей. Обучение рекуррентных сетей является трудной задачей из-за
проблемы взрывающихся и затухающих градиентов. Более того, RNN не могут
удерживать в "памяти" долговременные зависимости. Чтобы решить эту
проблему были предложены LSTM сети, которые способны удерживать
информацию в течение длительного времени. LSTM имеет три фильтра,
которые помогают защищать и контролировать состояние ячейки. Фильтр
входа (input gate) решает, какие значения следует обновить при подаче
новой информации. Фильтр "забывания" (forget gate) определяет какую
часть значений следует "забыть" и фильтр выхода (output gate) определяет
какую часть значения можно вывести в качество выхода.\

Шаг обновления LSTM:\

$$\begin{aligned}
i_t = &\: \sigma(\mathbf{W_i}e_t+\mathbf{U_i}h_{t-1}+b_i)\\
f_t = &\: \sigma(\mathbf{W_f}e_t+\mathbf{U_f}h_{t-1}+b_f)\\
o_t = &\: \sigma(\mathbf{W_o}e_t+\mathbf{U_o}h_{t-1}+b_o)\\
g_t = &\: \tanh(\mathbf{W_g}e_t+\mathbf{U_g}h_{t-1}+b_g)\\
c_t = &\: f_t\odot c{t-1}+i_t\odot g_t\\
h_t = &\: o_t \odot \tanh(c_t)\end{aligned}$$ где $\mathbf{W}$,
$\mathbf{U}$ - обучаемые матрицы параметров, $e_t$ - входной вектор,
$h_t$ - выходной вектор, $c_t$ - вектор состояния сети, $f_t$, $i_t$,
$o_t$ - векторы вентилей.\

В работе [@luInsiderThreatDetection2019] Каждый отдельный тип действия
пользователей кодируется определенным числом. Последовательность
закодированных действий разбивается на пересекающиеся блоки размера $n$
с помощью скользящего окна. Размер блоков является гиперпараметром.
Далее последовательность блоков действий подается н вход LSTM-сети,
которая пытается предсказать следующее действие пользователя.
Предполагается, что при нормальном поведении пользователя, правильное
действие пользователя попадает в $g$ наиболее вероятных предсказаний.
$g$ также является гиперпараметром.\

В работе проведены эксперименты, которые показывают, что LSTM
действительно обучается паттернам поведения пользователей и находить
аномальное поведение. Наилучший результат при подборе гиперпараметров:
точность - $0.84$ и полнота - $0.60$.\

В статье приводится предположение, что некоторые случаи, в которых
модель ошибочно не распознала аномальное поведение, можно разрешить с
помощью анализа контента.\

В работе [@brdiczkaProactiveInsiderThreat2012b] используется информация
о социальном графе для нахождения аномалий в нем и поведенческая
информация пользователей для психологического профилирования. Интересно
отметить, что в данной работе в качестве данных использовались данные из
популярной онлайн многопользовательской игры World of Warcraft(WoW). Они
собрали данные о социальном графе игроков внутри игры и его изменениях
за шесть месяцев. Авторы с помощью своего методы пытались предсказать
то, что игрок в скором времени покинет гильдию (социальную группу внутри
игры). В пользу необычного выбора набора данных приводится, что данных
много, содержат в себе зловредные поведения, публичны и не ограничены
правилами конфиденциальности. Авторы утверждают, что предложенный ими
метод можно применять также и на реальных предприятиях.\

В статье [@yuanInsiderThreatDetection2018b] используется следующий
подход: сначала LSTM выучивает поведение пользователя по его действиям и
извлекает временные признаки, затем извлеченные признаки подаются на
вход CNN классификатору.\

**Сверточные нейронные сети CNN** - специальная разновидность архитектур
нейронных сетей, в которой слои, выполняющие свертку, чередуются со
слоями субдискретизации. На данный момент, CNN является одним из лучших
алгоритмов по распознаванию и классификации изображений.\

Поведения пользователя рассматривается как последовательность действий и
действия одного пользователя соответствует одному \"предложению\", как в
обычных NLP задачах. Все действия пользователя перед подачей на вход
модели one-hot кодируются. После подачи каждого действия каждый скрытый
слой LSTM выдает вектор, который выражает текущее состояние сети в
пространстве малой размерности. Выходы последнего слоя собираются в одну
матрицу, которая затем приводится к фиксированному размеру с помощью
отбрасывания лишних векторов в случае длинных последовательностей
действий и заполнения нулями в случае коротких последовательностей.
Полученная матрица подается на вход CNN сети, которая в свою очередь
предсказывает аномальность поведения. Авторы пишут, что их метод показал
$AUC ROC=0.9449$ на отложенной выборке CERT.\

В другой работе [@saaudiInsiderThreatsDetection2019] используется
обратный подход. В ней CNN с одномерными сверточными слоями сначала
пытается извлечь признаки, затем они подаются на вход LSTM для
классификации. Значимое отличие от предыдущей работы заключается в том,
что отображение поведения пользователей происходит с помощью
представления каждого отдельного действия вектором малой размерностью.
Значения этого вектора характеризуются соседними действиями, которые
обычно встречаются вместе с ним. Авторы не указывают точно какой метод
они использовали, но по описанию это очень похоже на популярный подход
word2vec [@mikolovEfficientEstimationWord2013a]. Также авторы
использовали технику SMOTE [@chawlaSMOTESyntheticMinority2002] для
семплирования объектов малого класса и исправления сильного дисбаланса
классов в наборе данных. SMOTE генерирует синтетические данные, которые
похожи на $k$ ближайших соседей малого класса.\

В работе [@yuanAttentionBasedLSTMInsider2019] исследуется применение
механизма **Attention** (внимание) для задачи обнаружения инсайдеров.
Этот механизм позволяет сети обращать особое внимание для некоторых
важных действий. Attention-слой в данной сети собирают выходы LSTM-слоя
в один вектор, присваивая различный вес каждому выходу в зависимости от
его важности. Вычисления в Attention-слоя следующие:\

$$\begin{aligned}
\mathbf{u_t} = &\: \tanh (\mathbf{W_a}\mathbf{h_t}+\mathbf{b_a})\\
\alpha_t = &\: \frac{\exp (\mathbf{u_t^T}\mathbf{u_a})}{\sum_i \exp (\mathbf{u_i^T}\mathbf{u_a}}\\
\mathbf{v} = &\: \sum_t \alpha_t\mathbf{h_t}\end{aligned}$$ где
$\mathbf{W_a}$ - обучаемая матрица параметров, $\mathbf{u_a}$ -
обучаемый вектор контекста, $\mathbf{h_t}$ t-ый выход из LSTM-слоя.\
Полученный вектор $v$ подается на вход Softmax классификатору:
$$p(\hat{y}=k | \mathbf{v}) = \frac{\exp(\mathbf{W_k^T}\mathbf{v} + \mathbf{b_k})}{\sum^K_{i=1} \exp(\mathbf{W_i^T}\mathbf{v} + \mathbf{b_i})}$$
где, $\hat{y}$ - предсказанный класс, $K$ - количество классов,
$\mathbf{W_k}$ и $\mathbf{b_k}$ - обучаемые параметры для k-ого класса.\

Как показали эксперименты в [@yuanAttentionBasedLSTMInsider2019],
добавление Attention увеличивает AUC ROC для LSTM и RNN сетей при
использовании набора данных CERT.\

Для поиска аномалий также возможно применение нейронных сетей из области
распознавания изображений. Вдохновленные недавними успехами в применении
нейросетей для анализа изображений для классификации вредоносных
программ, [@gImageBasedFeatureRepresentation2019] применили этот подход
для задачи обнаружении инсайдерской угрозы. В этой работе из набора
данных CERT было вручную отобрано 20 признаков, и для каждого
пользователя отдельно по этим признаком составлялись изображения 32 на
32 пикселей, которые подавались предобученной на ImageNet популярным
нейросетевым моделям VGG16 и MobileNet. В результате было получено 99.32
точность и полнота на отложенной выборке.\

В [@leggAutomatedInsiderThreat2017] строится трехуровневая система для
обнаружения инсайдерских угроз. Первый уровень предполагает собой
профилирование поведения пользователей и целых ролей в совокупности. На
этом уровне срабатывает тревога при нарушении заданных правил.\

На следующем уровне для каждого пользователя собирается матрица дневных
наблюдений, размерность которой уменьшается с помощью PCA. Последняя
строка матрицы означает поведение пользователя в текущий день. В
качестве оценки аномальности берётся расстояние последней строки до
начала координат в новом пространстве. Вычисляется сразу множество
оценок аномальности, каждая из которых соответствует собственному
подмножеству используемых признаков и различному набору весов признаков.
Тревога на этом уровне срабатывает, если какая-либо из оценок
аномальности превышает заданные пороги.\

На третьем уровне анализируются полученные оценки аномальности, чтобы
понять, насколько эти оценки необычны в сравнении с обычным поведением
и, при необходимости, дать тревогу. Авторы предлагают для этого сразу
множество вариантов.\

Следует отметить, что в данной работе также учитывается контент файлов,
но лишь как дополнительный компонент системы. Авторы предлагают
использовать классический метод "мешок слов" или технику Linguistic
Inquiry and Word Count (**LIWC**)
[@tausczikPsychologicalMeaningWords2010], которая заключается в
распределение слов по категориям, имеющие смысл с точки зрения
психологии.\

В работе для обучения CERT, без указания точной версии, а для валидации
использовался их собственный синтетический набор данных, похожий по
структуре на CERT.\

Несмотря на то, что набор данных CERT кроме информации о совершенных
действиях пользователя также содержит сгенерированный контент файлов,
писем и веб-сайтов, в подавляющем большинстве работ контент файлов не
анализируется или используется лишь как дополнительный модуль, как
например в [@leggAutomatedInsiderThreat2017], где контент анализировался
с помощью техники \"мешок слов\". Это обусловлено тем, что в наборе
данных CERT до пятой версии генерации происходила с помощью "мешка слов"
по смеси тем пользователя. То есть контент представляет собой
неупорядоченный набор ключевых слов, которые которые представляют темы,
интересующие пользователя. Поэтому использовать техники, учитывающие
порядок слов, как например N-граммы, не имеет смысла.\

Также, как показано в статье [@yuanAttentionBasedLSTMInsider2019],
техника attention в применении к поведенческой информации пользователя,
способна значительно улучшить качество работы модели, позволяя модели
давать разный вес элементам последовательности в зависимости от важности
этого элемента. Это открывает потенциал для работы с семейством моделей
BERT [@devlinBERTPretrainingDeep2019], чей успех в NLP задачах
приписывается слоям-трансформерам, которые, в свою очередь, используют
attention. Поэтому с помощью модели BERT можно анализировать
последовательность действий пользователя.\

В [@leEvaluatingInsiderThreat2018] представлен подход представления данных для последющего анализа, который заключаются в использовании как _последовательных_ данных (последовательность действий конкретного пользователя за некоторый период времени), так и _численных_ данных. Численные данные в данной работе делятся на пользовательские и данные о действиях. Пользовательские данные в наборе данных CERT представлены информацией о роли пользователя в подразделении, его отделе и психометрике. Данные о действиях получаются подсчетом количества совершенных действий одной категории за рассматриваемый промежуток времени. Контентные данные набора данных CERT не были рассмотрены по силу синтетической природы набора данных.

Архитектуры рекуррентных сетей изначально предполагают работу только с последовательными данными. Чтобы совместить последовательные и численные данные можно использовать подход предложенный в [@karpathyDeepVisualSemanticAlignments2015]. В данной статье решалась задача автоматической аннотации изображений, которая состоит в выделением некоторых участков изображения с различными объектами и генерация текстового описания объеков в этих участках. Для генерации текста использовалась архитектура RNN, в которой начальное состояние скрытых слоев зависит от информационного вектора о соответствующем участке изображения. Таким образом, все состояния рекуррентной сети обусловлены содержанием изображения. Этот подход успешно решал поставленную задачу.
Нетрудно расширить этот подход для произвольного числового вектора $\overrightarrow{x}$, который также называется _условным_. Мы можем преобразовать условный вектор так, чтобы он совпадал по размеру с вектором скрытого состояния рекуррентной сети $\overrightarrow{h}$. Для этого достаточно применить следующее простое преобразование:

$$\overrightarrow(h_0) = W\overrightarrow(x) + \overrightarrow(b)$$

,где $W$ и $\overrightarrow(b)$ являются обучаемыми параметрами. Затем, полученным вектором можно инициализировать скрытое состояние рекуррентой сети. Процесс повторяется для каждой обрабатываемой рекуррентной сетью последовательностью.

В статье [@leAnalyzingDataGranularity2020b] на наборе данных сравниваются модели с различными рассматривамыми периодами для каждого пользователя: день, неделя и пользовательская сессия. Для данного периода собирались числовые данные о частоте событий и их статистические признаки, такие как среднее, медиана и стандартное отклонение. В результате исследования модели, которые были обучены на полных пользовательских сессиях дали лучший результат.

LSTM также используется в Unsupervised-режиме в работе [@paulLACLSTMAUTOENCODER]. В данной работе рассматривается обучение LSTM-автокодировщика. Также авторы в данной работе разбивают всех пользователей в наборе данных CERT по 8 непересекающимся сообществам. Авторы предлагают находить потенциальных инсайдеров по тому, насколько они выделяются внутри своего сообщества. Эксперименты показали, что таким способом можно успешно найти всех инсайдеров среди пяти наиболее аномальных пользователей из каждого сообщества. Эксперименты проводились на наборе данных CERT v6.2.

В работе [@tuorDeepLearningUnsupervised2017] используется похожий Unsupervised-подход с LSTM. Модель была обучена на задаче предсказания следующего действия в последовательности. Для определения аномальности используется отклонения действий пользователя от предсказанных моделью. Авторы называют главными преумеществами своего подхода интерпретируемость результатов модели и возможность онлайн-обучения.